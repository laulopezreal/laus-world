### AI summary (Raw notes + transcript)
  
   Use Case Interview Summary

**Team:** Computational Chemistry (Andrey Frolov & Christian Tyrchan)  
**Date:** [insert date]  
**Attendees:** Andrey Frolov, Christian Tyrchan, Richard Andersson, Laura Lopez-Real

#### üéØ Objective

Understand the Computational Chemistry team‚Äôs current compute environment, workflows, and pain points ‚Äî particularly in relation to GPU access, simulation workloads, and potential fit for **Polaris** or future HPC/GPU infrastructure.
#### 1. **Team Overview & Focus**

- The group runs **molecular dynamics (MD)** and **computational chemistry simulations** at scale.
    
- Primary focus: accelerating simulations for protein systems and drug-like molecules.
    
- **Key challenge:** limited GPU compute capacity; most workloads currently run on CPU clusters.
    
- They use **>100 open-source chemistry tools**, not internally developed.

**Current Compute Environment**

|Component|Details|
|---|---|
|**Hardware**|Mix of ARM-based CPUs and x86 Unix machines|
|**GPU Resources**|~40 GPUs locally; AWS cluster with 200‚Äì400 GPUs (on-demand)|
|**CPU Utilization**|~90% usage ‚Äî they try to parallelize workloads across CPU nodes|
|**HPC Environment**|Use of **Scientific Compute Platform (SCP)** modules for simulation tools|
|**Storage**|~2 TB per user quota; total departmental capacity ~100 TB ‚Üí frequently run out of space|
|**Compute Nodes**|Dedicated Unix machines for main workflows|
|**Cluster Management**|Limited DevOps practices; no complex orchestration pipeline|
|**External Compute**|Pilot testing AWS-based HPC for large jobs|
**Software Stack & Tooling**

- Rely primarily on **open-source simulation tools** available as modules in the **SCP**.
    
- Example tools include molecular dynamics, quantum chemistry, and structure analysis packages (not developed in-house).
    
- Use of **Promox** (internal or related) ‚Äî explored by postdocs for performance acceleration.
    
- Software environment and dependencies managed through SCP modules, with minimal containerization or DevOps automation.
4. **Performance & Computational Bottlenecks**

- **Current limitation:** insufficient GPU power to train or run large models efficiently.
    
- **Workaround:** parallelization across CPUs, though this scales poorly for large simulations.
    
- **Potential breakthrough:** if simulations could be GPU-accelerated effectively, training times could be reduced from _months to weeks_ for microsecond-scale trajectories.
    
- **Utilization goal:** maintain near-100% GPU/CPU usage for cost efficiency (‚Äúevery last drop‚Äù).
    

---

5. **Workflow Summary**
- Simulations and modeling are typically executed via SCP modules or Unix scripts.
    
- Data handled manually; limited automation or CI/CD practices.
    
- No mention of Slurm or similar job schedulers ‚Äî workflows are relatively flat and team-driven.
    
- Postdocs are often engaged in exploring optimization techniques or new simulation pipelines.

5. **Data Characteristics**
- Simulation data sizes range from several GBs to multiple TBs per project.
- Larger systems (e.g., membrane proteins) require significantly more time and space.
    
- **Typical throughput:**
    
    - Small proteins ‚Üí ~500 ns/day
        
    - Larger systems ‚Üí 50‚Äì200 ns/day
        
    - Targeting microsecond-scale simulations (currently take _months_).
        

7. **Polaris & Future Alignment**
- Open to leveraging **GPU-based compute** for simulation acceleration and ML model training.
    
- Would benefit from:
    
    - **Multi-GPU job support** and elastic scaling.
        
    - **Simplified data management** and quota expansion.
        
    - **Centralized access** to pre-installed open-source chemistry libraries.
        
- They are **ideal early adopters** for GPU-intensive simulation workloads in Polaris.
    
üí° Key Insights / Opportunities

|Area|Observation|Potential Next Step|
|---|---|---|
|**Compute Power**|CPU-heavy environment; limited GPUs|Enable GPU-based simulation queue in Polaris|
|**Data Storage**|Departmental space limit (100 TB) reached frequently|Expand storage or connect to scalable S3-compatible system|
|**DevOps Maturity**|Minimal automation|Provide reusable workflow templates or managed pipelines|
|**Open-Source Tools**|Heavy reliance on external software|Pre-package key chemistry modules in Polaris environment|
|**Performance Goals**|Aim to shorten simulation cycles by 3‚Äì4√ó|Benchmark GPU-accelerated simulations on Polaris test nodes|

üß© Summary Quote (from transcript)

> ‚ÄúFor relatively small proteins, we can reach ~500 ns/day ‚Äî but large systems with membranes take months. If we could bring that down to weeks, it would be massive.‚Äù
  
### Raw notes
**Andrey Frolov and Christian Tyrchan**

Arm based cpu

90% CPU

Now they need GPU

They have 40 GPUs

They try to parallelise in CPU

They have a an AWS cluster with 200-400 GPUs

They have over 100 chemical tools

Many different softwares to run simulations


These tools are not developed internally and they are open source. Christian says that AZ has not enough computational power to train models.

  

Currently they use programs that are available in the SCP as modules that they can load

Promox - There are some postdocs working on these simulations, in case they can speed the training, it would be pretty disrupting

  

They dont have a very complex devops workflow

They have 3 main places where they run their workflows

- Dedicated Unix machines

  

2T of space per user, they have 100T for the whole department and they often run out space.

  

AZ Serve